import streamlit as st
from groq import Groq
from tavily import TavilyClient
import edge_tts
import asyncio

# --- 1. Configura√ß√£o ---
st.set_page_config(page_title="Jarvis Aut√¥nomo", page_icon="üß†")
st.title("Assistente Aut√¥nomo (Auto-Internet)")

# --- 2. Conex√£o ---
try:
    client = Groq(api_key=st.secrets["GROQ_API_KEY"])
    tavily = TavilyClient(api_key=st.secrets["TAVILY_API_KEY"])
except Exception as e:
    st.error("‚ö†Ô∏è Erro nas Chaves API. Verifique os Secrets.")
    st.stop()

# Modelo Principal (C√©rebro)
MODEL_ID = "llama-3.3-70b-versatile"
# Modelo R√°pido (Para decidir se busca ou n√£o - economiza tempo)
ROUTER_MODEL = "llama3-8b-8192" 

# --- 3. Mem√≥ria ---
if "memoria_v3" not in st.session_state:
    st.session_state.memoria_v3 = []
if "ultimo_audio" not in st.session_state:
    st.session_state.ultimo_audio = None

# Sidebar Limpa (Sem bot√£o de internet, agora √© autom√°tico)
if st.sidebar.button("üóëÔ∏è Limpar Tudo"):
    st.session_state.memoria_v3 = []
    st.session_state.ultimo_audio = None
    st.rerun()

# --- FUN√á√ïES INTELIGENTES ---

def cerebro_decisor(pergunta):
    """
    Esta fun√ß√£o √© o 'Router'. Ela decide SE precisa buscar na web.
    Retorna: True (Buscar) ou False (Responder direto)
    """
    system_prompt = """
    Voc√™ √© um classificador de inten√ß√£o. Analise a pergunta do usu√°rio.
    - Se a pergunta pedir dados em tempo real (cota√ß√µes, clima, not√≠cias, jogos, eventos recentes), responda 'BUSCAR'.
    - Se for conversa fiada, ajuda t√©cnica, c√≥digo, resumo ou conhecimento geral atemporal, responda 'RESPONDER'.
    Responda APENAS uma palavra: 'BUSCAR' ou 'RESPONDER'.
    """
    
    try:
        completion = client.chat.completions.create(
            model=ROUTER_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": pergunta}
            ],
            temperature=0
        )
        decisao = completion.choices[0].message.content.strip().upper()
        return "BUSCAR" in decisao
    except:
        return False # Na d√∫vida, n√£o busca

def buscar_tavily(pergunta):
    try:
        response = tavily.search(query=pergunta, search_depth="basic", max_results=3)
        contexto = []
        for r in response.get('results', []):
            contexto.append(f"- {r['title']}: {r['content']}")
        return "\n".join(contexto)
    except: return None

def ouvir_audio(audio_bytes):
    try:
        return client.audio.transcriptions.create(
            file=("temp.wav", audio_bytes, "audio/wav"),
            model="whisper-large-v3",
            response_format="text",
            language="pt"
        )
    except: return None

async def falar(texto):
    OUTPUT = "resposta.mp3"
    VOICE = "pt-BR-FranciscaNeural"
    await edge_tts.Communicate(texto, VOICE).save(OUTPUT)
    return OUTPUT

# --- 4. Interface ---
chat_container = st.container()
with chat_container:
    for m in st.session_state.memoria_v3:
        with st.chat_message(m["role"]):
            st.markdown(m["content"])

st.divider()
col1, col2 = st.columns([0.2, 0.8])

texto_input = None
falar_resposta = False

with col2:
    if txt := st.chat_input("Pergunte algo..."):
        texto_input = txt

with col1:
    if audio := st.audio_input("üéôÔ∏è"):
        if audio != st.session_state.ultimo_audio:
            st.session_state.ultimo_audio = audio
            with st.spinner("Ouvindo..."):
                if transcricao := ouvir_audio(audio):
                    texto_input = transcricao
                    falar_resposta = True

# --- 5. Fluxo Principal (O AGENTE) ---
if texto_input:
    # Mostra User
    st.session_state.memoria_v3.append({"role": "user", "content": texto_input})
    with chat_container.chat_message("user"):
        st.markdown(texto_input)

    with chat_container.chat_message("assistant"):
        placeholder = st.empty()
        dados_web = ""
        
        # --- PASSO 1: O C√âREBRO DECIDE ---
        with st.status("üß† Analisando sua pergunta...", expanded=True) as status:
            precisa_busca = cerebro_decisor(texto_input)
            
            if precisa_busca:
                status.write("üåç Decidi pesquisar na web!")
                raw_data = buscar_tavily(texto_input)
                if raw_data:
                    dados_web = f"\n\n[DADOS DA INTERNET]:\n{raw_data}\n"
                    status.update(label="‚úÖ Dados encontrados!", state="complete", expanded=False)
                else:
                    status.update(label="‚ùå Falha na busca (seguindo sem dados)", state="error")
            else:
                status.write("üìö Usando conhecimento interno.")
                status.update(label="‚úÖ Respondendo direto", state="complete", expanded=False)

        # --- PASSO 2: GERA RESPOSTA ---
        try:
            with st.spinner("Formulando resposta..."):
                # Monta o prompt com ou sem dados da web
                msgs = [{"role": "system", "content": "Voc√™ √© uma assistente prestativa. Se receber dados da internet, use-os. Se n√£o, use seu conhecimento."}]
                for m in st.session_state.memoria_v3[:-1]:
                    if m.get("content"): msgs.append({"role": m["role"], "content": str(m["content"])})
                
                msgs.append({"role": "user", "content": texto_input + dados_web})

                stream = client.chat.completions.create(model=MODEL_ID, messages=msgs, stream=False)
                resp = stream.choices[0].message.content
                placeholder.markdown(resp)
                
                # --- PASSO 3: FALA (Se necess√°rio) ---
                if falar_resposta:
                    audio_file = asyncio.run(falar(resp))
                    st.audio(audio_file, format="audio/mp3", autoplay=True)
                
                st.session_state.memoria_v3.append({"role": "assistant", "content": resp})
        
        except Exception as e:
            st.error(f"Erro: {e}")
